\chapter{Métodos de agrupamiento de datos}
Un método de agrupamiento de datos o método de ``clustering'', es un método de clasificación no supervisado que permite la partición de un conjunto de $N$ objetos en $K$ grupos o clases, de tal forma que los objetos miembro de un grupo sean más similares entre si (en algún sentido a definir) que entre los miembros de otros grupos.\\\\
Son métodos no supervisados ya que en un proceso de agrupación no existen clases definidas previamente ni ejemplos de que tipo de relaciones se desea encontrar entre los objetos, por lo que el mismo proceso debe generar las clases iniciales a las cuales asignar los objetos en el proceso de clasificación.\\\\
Estas técnicas permiten el descubrimiento o identificación de distribuciones y patrones subyacentes en los datos, posibilitando obtener conclusiones sobre los mismos, lo que las hace una de las herramientas más útiles en procesos de minería de datos y aprendizaje automatizado en campos tan diversos como las ciencias sociales, las ciencias médicas y la ingeniería.\\\\
Dependiendo de los criterios utilizados para realizar la partición, un proceso de agrupamiento puede resultar en diferentes particiones. Como ejemplo de esto podemos tomar el conjunto de números $\{-5, -3, -2, 2, 3\}$. Si decidimos agruparlos por su módulo, obtendremos los conjuntos $\{-5\}$, $\{-3, 3\}$, $\{-2, 2\}$, mientras que si decidimos agruparlos por positividad o negatividad, obtendremos los conjuntos $\{-5, -3, -2\}$ y $\{2, 3\}$. También podríamos haber optado por agrupar por paridad, si son o no primos, etc. Como se observa de un ejemplo tan sencillo, es de fundamental importancia la elección de las propiedades de los objetos a partir de las cuales realizar el agrupamiento.\\\\
En el presente trabajo nos interesará agrupar y caracterizar conjuntos de genes de un organismo modelo, la planta \textit{Arabidopsis thaliana}, en base a sus perfiles de expresión génica a lo largo de los diversos tratamientos presentados en el capitulo anterior.\cite{Gan2007, Halkidi2001, Domany1999}\\\\
Discutiremos a continuación diferentes metodologías y criterios de similaridad que pueden ser considerados para ello.
\section{Similaridad, distancia y disimilaridad}
Las distancias y similaridades tienen un rol preponderante en el análisis de agrupamiento de datos y por regla general son conceptos recíprocos.\\\\
Una medida de similaridad o coeficiente de similaridad se utiliza para indicar de forma cuantitativa la fuerza de la relación entre dos objetos del conjunto. Los $i = 1, 2, ...N$ objetos de un conjunto $E$ pueden ser definidos en términos de las coordenadas $\vec{Xi}$ de sus puntos representativos en un espacio $n-dimensional$. Sean $\vec{x}=\{x_0, x_1,...,x_n\}$ e $\vec{y}=\{y_0, y_1,...,y_n\}$ dos puntos $n-dimensionales$. Entonces, el coeficiente de similaridad entre ambos será una función de sus atributos:
\begin{equation}
	s(\vec{x}, \vec{y}) = s((x_0, x_1,...,x_n), (y_0, y_1,...,y_n))
\end{equation}
con $s$ una función simétrica, es decir, $s(\vec{x}, \vec{y}) = s(\vec{y}, \vec{x})$. Cuanto mayor es el coeficiente de similaridad, mayor es la similaridad entre ambos.\\\\
Por otro lado, las medidas de disimilaridad o de distancia se comportan de forma inversa, a mayor distancia o disimilaridad, más diferentes son dos puntos.
Una métrica de distancia es una función $d \in R$ definida sobre un conjunto $E$ que cumple las siguientes propiedades:
\begin{enumerate}
\item No-negatividad: $d(\vec{x}, \vec{y}) \geq 0$
\item Reflexividad: $d(\vec{x}, \vec{y}) = 0 \iff \vec{x} = \vec{y}$
\item Conmutatividad: $d(\vec{x}, \vec{y}) = d(\vec{y}, \vec{x})$
\item Desigualdad triangular: $d(\vec{x}, \vec{y}) \leq d(\vec{x}, \vec{z}) + d(\vec{z}, \vec{y})$
\end{enumerate}
con $\vec{x},\vec{y},\vec{z}$ objetos arbitrarios del conjunto.\\\\

Una medida de disimilaridad es una métrica si cumple con las propiedades antes enunciadas. \\\\
Aunque no pareciera existir una definición formal de métrica de similaridad, Chen y colaboradores definen en \cite{Chen2009} una métrica de similaridad como una función $s$ que cumple:
\begin{enumerate}
\item $s(\vec{x}, \vec{y}) = s(\vec{y}, \vec{x})$
\item $s(\vec{x}, \vec{x}) \geq 0$
\item $s(\vec{x}, \vec{x}) \geq s(\vec{x}, \vec{y})$
\item $s(\vec{x}, \vec{x}) = s(\vec{y}, \vec{y}) = s(\vec{x}, \vec{y}) \iff x=y$
\item $s(\vec{x}, \vec{y}) + s(\vec{y}, \vec{z}) \leq s(\vec{x}, \vec{z}) + s(\vec{y}, \vec{y})$
\end{enumerate}
La condición 5 indica que la similaridad entre $\vec{x}$ y $\vec{z}$ a través de $\vec{y}$ no es mayor que la similaridad directa entre $\vec{x}$ y $\vec{z}$ sumada a la autosimilaridad de $\vec{y}$. Esta propiedad es el equivalente de la desigualdad triangular para una distancia métrica.\\\\
Si bien es deseable que una similaridad o disimilaridad sea una métrica, existen muchas medidas de similaridad o disimilaridad que dan excelentes resultados en técnicas de agrupamiento de datos sin ser métricas, es decir, sin que necesariamente cumplan la desigualdad triangular o el item 5 de métrica de similaridad.\cite{Chen2009}\\\\
Finalmente, los objetos del conjunto pueden ser especificados por medio de una ``matriz de distancia'' de $N\times N$ cuyos elementos $d_{ij}$ indican la disimilaridad entre los puntos $i$ y $j$.\cite{Halkidi2001, Domany1999, Gan2007, Kheng2010}

\subsection{Medidas de distancia}
El análisis de datos de expresión génica se basa principalmente en la comparación de perfiles de expresión génica. Para poder comprarlos, se requiere una medida que cuantifique cuán similares o disimilares son los objetos considerados. La elección de una medida de distancia será entonces de fundamental importancia para lograr agrupamientos que tengan sentido en el contexto de los datos analizados. En las subsiguientes secciones se listarán las medidas de distancia más comúnmente utilizadas en el agrupamiento de datos (no necesariamente de datos de perfiles de expresión).
\subsubsection{Distancia euclidiana}
La distancia euclidiana es probablemente la distancia más utilizada en el contexto de datos numéricos.\\
Para dos puntos $\vec{x}$ e $\vec{y}$ en un espacio $n-dimensional$, la distancia euclidiana se define como:
\begin{equation}
	d_{euc}(\vec{x}, \vec{y}) = [\sum\limits_{i=1}^n (x_i-y_i)^2]^\frac{1}{2} = [(\vec{x}-\vec{y})(\vec{x}-\vec{y})^T]^\frac{1}{2}
\end{equation}
con $x_i$ e $y_i$ los valores de la \textit{i}esima componente de $\vec{x}$ e $\vec{y}$ respectivamente.

\subsubsection{Distancia Manhattan o Taxicab}
La distancia Manhattan o taxicab es llamada así por ser la distancia que debería recorrer un taxi en una ciudad para ir de un punto a otro, suponiendo la ciudad como una cuadrícula perfecta.\\
Para dos puntos $\vec{x}$ e $\vec{y}$ en un espacio $n-dimensional$, la distancia Manhattan se define como:
\begin{equation}
	d_{man}(\vec{x}, \vec{y}) = \sum\limits_{i=1}^n |(\vec{x}-\vec{y})|
\end{equation}
\subsubsection{Distancia máxima}
Para dos puntos $\vec{x}$ e $\vec{y}$ en un espacio $n-dimensional$, la distancia máxima se define como:
\begin{equation}
	d_{max}(\vec{x}, \vec{y}) = \max_{1 \leq j \leq n} |x_i-y_i|
\end{equation}
\subsubsection{Distancia de Minkowsky}
Para dos puntos $\vec{x}$ e $\vec{y}$ en un espacio $n-dimensional$, la distancia de Minkowsky se define como:
\begin{equation}
	d_{mink}(\vec{x}, \vec{y}) = [\sum\limits_{i=1}^n (x_i-y_i)^r]^\frac{1}{r}, r \geq 1
\end{equation}
$r$ es el orden de la distancia de Minkowsky. Notar que si tomamos $r = 2,1,\inf$ obtenemos la distancia euclidiana, la Manhattan y la máxima, respectivamente.
\subsubsection{Coeficiente de correlación de Pearson}
Una de las métricas más utilizadas para medir similaridad entre perfiles de expresión, como los presentados en la figura \ref{fig:perfiles_sin_agrupar}, es el coeficiente de correlación de Pearson \cite{Babu2004}.\\\\
El coeficiente de correlación fue desarrollado por Karl Pearson basado en ideas introducidas por Francis Galton alrededor del año 1880.\\\\
\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.6\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{perfiles_sin_agrupar}
    \caption{Perfiles de expresión de cinco genes tomados al azar.}
    \label{fig:perfiles_sin_agrupar}
    \end{subfigure}    
    
    \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{perfiles_anti_coregulados}
    \caption{Perfiles de expresión de 10 genes que están co-expresados y anti co-expresados.}
    \label{fig:perfiles_anti_coregulados}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{perfiles_coregulados}
    \caption{Perfiles de expresión de 10 genes que están co-expresados.}
    \label{fig:perfiles_coregulados}
    \end{subfigure}
    \caption{Distintos grupos de perfiles de expresión para el tratamiento \textit{Control}. Cada gen está representado por un color y un número. Cada punto indica una medición del nivel de expresión del gen. Los puntos están unidos por lineas para mejorar su visualización.}
\end{figure*}
Para dos puntos $\vec{x}$ e $\vec{y}$ en un espacio $n-dimensional$, representando el perfil de expresión de dos genes a lo largo de un dado tratamiento, el CCP se define como:
\begin{equation}
	r(\vec{x}, \vec{y}) = \frac{\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{[\sum\limits_{i=1}^n(x_i-\bar{x})^2]^\frac{1}{2}[\sum\limits_{i=1}^n(y_i-\bar{y})^2]^\frac{1}{2}}
	\label{eq:ccp}
\end{equation}
o de forma equivalente:
\begin{equation}
	r(\vec{x}, \vec{y}) = \frac{\frac{1}{n-1}\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{s_x s_y}
\end{equation}
con $s$ la desviación estandar de la muestra. El centrar alrededor de la media permite comparar la forma de ambos perfiles, en lugar de su magnitud.\\\\
Valores altos de $r$ implican que las fluctuaciones respecto de la media de las respectivas componentes se encuentran ``en sincronia''. En caso de no estarlo el valor esperado tiende a cero. Finalmente si las fluctuaciones tienden a ocurrir ``sincronizadamente'' pero en sentidos opuestos $r \Rightarrow -1$.\\\\
En nuestro caso, $r=+1$ corresponde a genes que estan siendo coexpresados por la maquinaria celular, mientras que $r=-1$ a genes anti-coexpresados. En el primer caso los perfiles de ambos genes
(apropiadamente reescaleados) coinciden perfectamente, mientras que en el segundo son perfectamente opuestos.\\\\
La correspondiente medida de distancia puede ser calculada como \cite{Dhaeseleer2005}:
\begin{equation}
	d_{ccp}(\vec{x}, \vec{y}) = 1-r(\vec{x}, \vec{y})
	\label{eq:distancia_correlacion}
\end{equation}
o alternativamente:
\begin{equation}
	d_{ccp}(\vec{x}, \vec{y}) = 1-|r(\vec{x}, \vec{y})|
	\label{eq:distancia_correlacion_absoluta}
\end{equation}
En el caso de la distancia definida en \ref{eq:distancia_correlacion_absoluta}, al tomar el valor absoluto del CCP, genes cuyos perfiles son iguales pero opuestos (están anti-coexpresados) pueden encontrarse más cerca en el sentido de $d_{ccp}$ que aquellos que son expresados hacia arriba o abajo pero en distintas magnitudes. Por lo tanto, esta distancia permite encontrar grupos de genes que son coexpresados, sin importar en que sentido (Figura \ref{fig:perfiles_anti_coregulados}) sean coexpresados.
En el caso de la distancia definida en \ref{eq:distancia_correlacion}, solamente se consideran cercanos aquellos genes cuyos perfiles sean coexpresados o bien hacia arriba o bien hacia abajo (Figura \ref{fig:perfiles_coregulados}).\cite{Hennig2013, Kheng2010, Babu2004, Gan2007}\\\\
En el presente trabajo se utilizará como distancia la definida en \ref{eq:distancia_correlacion} para encontrar grupos de genes que únicamente se hayan coexpresado o bien hacia arriba o bien hacia abajo.\cite{Eisen1998}
\clearpage
\subsection{Similaridad semántica}
La adopción de ontologías provee los medios para comparar aspectos de entidades que de otra forma no podrían ser comparados. Por ejemplo, si dos productos génicos son anotados dentro del mismo esquema, es posible compararlos mediante el análisis de los términos en los cuales están anotados de forma explícita utilizando medidas de similaridad semántica. Se define una medida de similaridad semántica como una función tal que dados dos términos de la ontología o un conjunto de términos en los que dos genes están anotados, la función devuelve un escalar que refleja la cercanía de sentido entre ellos.\\\\
Es posible cuantificar la similaridad semántica en una ontología representada por un grafo como GO, mediante diversas estrategias.
\subsubsection{Comparación de términos en GO}
Sea $C$ el conjunto de todos los términos de una ontología GO, con un número total $\#C$ de anotaciones. Un término $c_i$ tendrá $\#c_i$ anotaciones, ya sea directamente o por intermedio de cualquiera de sus hijos. La probabilidad de que un gen tomado al azar, sin otro tipo de información, se encuentre anotado al concepto $c_i$ será entonces $P(c_i) = \frac{\#c_i}{\#C}$, con $P:C\Rightarrow [0,1]$.\\\\
Se define el contenido de información de $c_i$ como $IC = -log_2(P(c_i))$, cantidad en el intervalo $(0, -log_2[\frac{1}{\#C}])$, que indica cuán específico e informativo es un término de la ontología. Para un $c_i$ y $c_j$ tales que $c_i \preceq c_j$, se tiene que $IC(c_i) \geq IC(C_j)$. Cuanto más específico sea un término, es menos probable que un gen dado esté anotado en el mismo, y por lo tanto, su contenido de información es mayor. El nodo raíz de la ontología tiene un contenido de información nulo, ya que es el ancestro de todos los términos de la misma y por lo tanto, saber que un concepto está anotado a la raíz no aporta información.\\\\
Es posible definir una medida entre pares de términos utilizando el IC. Una de las medidas de similaridad semántica más comúnmente utilizadas es la medida de similaridad semántica introducida por Resnik en \cite{Resnik1995}, que consiste en asignar como la medida de similaridad entre dos términos, el contenido de información del ancestro en común más informativo (el MICA):
\begin{equation}
	Sim_{res}(c_i, c_j) = \max\limits_{c \in S(c_i, c_j)}(-log_2[P(c)]) = IC(MICA[c_i, c_j])
\end{equation}
Con $S(c_i, c_j)$ el conjunto de ancestros comunes de $c_i$ y $c_j$. De esta manera, para cuantificar la información compartida (y estimar entonces su similaridad semántica) se considera el contenido de información de los ancestros en común que dos términos poseen.\\\\
A modo de ejemplo, tomemos el DAG de la figura \ref{fig:dag}, con 9 términos o conceptos: $C=\{R, c_0,...,c_7\}$ y con 5 entidades mapeadas (genes anotados): $g_1=\{5, 6, 2, 0, r\}$, $g_2=\{5, 4, 2, 3, 0, r\}$, $g_3=\{7, 1, r\}$, $g_4=\{4, 3, 0, r\}$ y $g_5=\{2, 0, r\}$. Podemos calcular la similaridad semántica de Resnik entre los términos $c_4$ y $c_5$, por ejemplo, sabiendo que $\#C = 5$ y que el ancestro común más informativo de ambos es $c_0$, con $\#c_0=4$. Se tiene entonces que $Sim_{res}(c_4, c_5) = IC(MICA) = IC(c_0) = -log_2(\frac{\#c_0}{\#C}) = -log_2(\frac{4}{5}) = 0.32$. Si quisieramos calcular ahora la similaridad semántica Resnik entre $c_5$ y $c_6$, obtendríamos $Sim_{res}(c_5, c_6) = IC(c_2) = -log_2(\frac{3}{5}) = 0.73$. Por lo tanto, para Resnik, los conceptos $c_5$ y $c_6$ son entre sí, más similares que los conceptos $c_4$ y $c_5$.\\\\
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{dag}
    \caption{DAG con 9 términos o conceptos: $C=\{R, c_0,...,c_7\}$ y con 5 entidades mapeadas (genes anotados): $g_1=\{5, 6, 2, 0, r\}$, $g_2=\{5, 4, 2, 3, 0, r\}$, $g_3=\{7, 1, r\}$, $g_4=\{4, 3, 0, r\}$ y $g_5=\{2, 0, r\}$}
    \label{fig:dag}
\end{figure}
Existen más de dos docenas de medidas de similaridad entre términos GO, y no siempre es claro cuál es el mejor para un dado propósito. Sin embargo, generalmente la elección de una medida por defecto es suficiente\cite{Bose2016}. En este trabajo utilizaremos la $Sim_{res}$, por tratarse de una medida simple y efectiva.\\\\
Una vez establecida una medida de similaridad semantica entre termino GO, existen distintas formas para extender esta idea y definir una similaridad semántica entre genes. Básicamente existen 2
estrategias. La primera se basa en medidas globales (groupwise en inglés), que comparan globalmente los conjuntos de términos en los que dos genes están anotados, $GO(g_1)$ y $GO(g_2)$, por ejemplo, contando cuantos términos comparten: $|GO(g_1) \cap GO(g_2)|$.\cite{Lee2004} \\\\
La segunda estrategia se basa en medidas de a pares (pairwise en inglés), calculando la similaridad semántica término a término de cada uno de los conjuntos $GO(g_1)$ y $GO(g_2)$ y luego aplicando sobre esta similaridad alguna operación para obtener una medida de similaridad entre estos genes.\\\\
El primer paso para esto es calcular una matriz de similaridad $S$ de $N\times M$ que contenga la similaridad de a pares, entre todos los pares de términos de estos conjuntos, con $N=|GO(g_1)|$ y $M=|GO(g_2)|$, utilizando alguna de las medidas de similaridad semántica entre términos, por ejemplo, $Sim_{res}$:

\begin{equation}
	S(g_1, g_2)_{ij} = Sim(GO(g_1^i), GO(g_2^j)), \forall i \in \{1,...,N\} y \forall j \in \{1,...,M\}
\end{equation}
Notar que esta matriz puede no ser simétrica.\\\\
Cada una de las $N$ filas corresponde a la similaridad entre la anotación $i-esima$ del gen $1$ y todas las $M$ anotaciones del gen $2$ y cada una de las $M$ columnas corresponde a la similaridad entre la anotación $j-esima$ del gen $2$ y todas las $N$ anotaciones del gen $1$.\\\\
A partir de $S_{ij}$ es posible definir tres métodos para obtener una medida de similaridad entre genes.
El primer método, propuesto en \cite{Sevilla2005}, consiste en tomar como similaridad, la máxima similaridad entre todos los términos:
\begin{equation}
	Sim_{max}(GO(g_i), GO(g_j)) = \max\{S_{ij}\}
\end{equation}
El segúndo método, propuesto en \cite{Lord2003}, consiste en tomar el valor medio de todos los valores de la matriz $S_{ij}$:
\begin{equation}
	Sim_{med}(GO(g_i), GO(g_j)) = \frac{1}{N.M}\sum\limits_{i,j}S_{ij}
\end{equation}
Finalmente, el tercer método, propuesto en \cite{Pesquita2009}, implica tomar el valor medio de los máximos de cada fila, el valor medio de los máximos de cada columna, y quedarse con el máximo de esos dos valores. Este criterio de similaridad se conoce como \textit{rcmax}:
\begin{equation}
	Sim_{rcmax}(GO(g_1), GO(g_2)) = \max\{\frac{1}{N}\sum\limits_{i}\max\limits_{1\leq j \leq M}S_{ij}, \frac{1}{M}\sum\limits_{j}\max\limits_{1\leq i \leq N}S_{ij}\}
	\label{eq:sim_rcmax}
\end{equation}
Como muchos genes están anotados en conceptos muy diversos por participar en procesos biológicos muy distintos, e incluso puede haber genes que no están anotados en ningún concepto, la medida de similaridad $Sim_{med}$ tiende a dar valores más bajos que otros métodos. Por el contrario, la medida $Sim_{max}$ tiende a dar valores más altos, por ser una medida más optimista. En este trabajo utilizaremos el tercer método, $Sim_{rcmax}$, por ser un compromiso entre ambos casos extremos.
\cite{Resnik1995, Pesquita2009, Bose2016, Lin1998, Jiang1997, Sevilla2005, Lord2003}

\section{Estrategias de agrupamiento}
En la sección anterior abordamos distintas metodologías para cuantificar la noción de similaridad en diversos espacios.\\\\
En lo que sigue introduciremos las diferentes estrategias de agrupamiento de datos utilizadas en este trabajo, tanto para agrupamiento de perfiles transcripcionales como de armado de comunidades en las redes presentadas anteriormente.\\\\
Es posible distinguir dos tipos de agrupamientos, conocidos como agrupamiento duro (hard clustering en inglés), y agrupamiento difuso (fuzzy clustering en inglés). En el primer caso, el de agrupamiento duro, cada objeto del conjunto de datos es asignado a un y solo un grupo, mientras que en el segundo caso, el de agrupamiento difuso, un elemento del conjunto puede pertenecer a varios grupos, con distinta probabilidad. En este trabajo utilizaremos únicamente métodos de agrupamiento duro.\\\\
\section{Agrupamientos no jerárquicos}
\label{sec:agrupamientos_no_jerarquicos}
Además de la distinción mencionada más arriba, los métodos de agrupamiento pueden dividirse (entre otros) fundamentalmente entre agrupamientos jerárquicos y agrupamientos no jerárquicos.
Las dos estrategias de agrupamientos no jerárquicos que se presentan a continuación fueron utilizadas en el desarrollo de este trabajo.
\subsection{K-means}
K-means es un método usual de agrupamiento no jerárquico en donde cada observación pertenece al grupo con la media más cercana a la observación, y donde $K$, el número de grupos, debe ser fijado a priori y puede ser determinado arbitrariamente o estimado a partir de algún factor de merito apropiado.\cite{Hartigan1979}\\\\
Más formalmente, sea un conjunto de observaciones $\{\vec{x_1},...,\vec{x_n}\}$, k-means construye una partición de las observaciones en $k$ grupos con $k \leq n$ a fin de minimizar una función de costo, como ser la suma de los cuadrados dentro de cada grupo $G = \{g_1,...,g_k\}$: 
\begin{equation}
	C = argmin\sum\limits_{i=1}^k \sum\limits_{x_j \in g_i}||x_j-\mu_i||^2
\end{equation}
Con $\mu_i$ el valor medio de los elementos del grupo $g_i$.
La figura \ref{fig:ejemplo_k_means} muestra un conjunto de observaciones y los grupos que se obtienen fijando $k=2$ y $k=5$, junto con sus respectivos centroides.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{ejemplo_k_means}
    \caption{Agrupamiento de ejemplo para un conjunto de puntos bidimensionales aleatorios utilizando k-means con $k=2$ y $k=5$. Cada punto está representado por un número y un color indicando su grupo de pertenencia.}
    \label{fig:ejemplo_k_means}
\end{figure}
Se observa que dependiendo del $k$ utilizado, el algoritmo encuentra particiones con mayor o menor nivel de \textit{resolución}. Volveremos sobre el tema de la resolución más adelante.
\cite{Kogan2006}

\subsection{PAM}
Si bien k-means es uno de los métodos de partición más utilizados ya que es muy eficiente en términos de tiempo computacional, el mismo es muy sensible a observaciones aisladas. Por esta razón, en algunos métodos se reemplazan los centroides, que son puntos no necesariamente pertenecientes al conjunto de observaciones, por medoides, que son los objetos más centrales dentro del grupo (se reemplaza k-means por k-medoids). Esto hace que el método sea insensible a observaciones aisladas.\\\\
Particionar alrededor de medoides (Partitioning around medoids en inglés) es uno de los métodos más conocidos que hace uso de este concepto, buscando minimizar la función de costo:
\begin{equation}
	C = argmin\sum\limits_{i=1}^k \sum\limits_{x_j \in g_i} d(x_j, m_i)
\end{equation}
Con $m_i$ el medoide del grupo $i$ y $d(x_j, m_i)$ la distancia entre el objeto $x_j$ del grupo $i$ y el medoide del mismo grupo.
\cite{Park2009, Ibrahim2012}
\section{Agrupamientos jerárquicos}
Existen dos acercamientos distintos para realizar un agrupamiento jerárquico: se puede ir ``desde abajo hacía arriba'', agrupando grupos más chicos en grupos más grandes, lo que se conoce como agrupamiento aglomerativo, o se puede ir ``desde arriba hacia abajo'', dividiendo grupos más grandes en grupos más chicos, lo que se conoce como agrupamiento divisivo. En este trabajo nos interesará únicamente trabajar con agrupamientos aglomerativos.\\\\
Un agrupamiento jerárquico aglomerativo comienza con cada objeto en un grupo separado. Luego, se unen los dos grupos más cercanos de acuerdo a algún criterio definido generando un nuevo grupo a partir de ambos. Al nuevo grupo se le asignará una distancia al resto de los grupos de acuerdo a cierto criterio. Esto se repite hasta que solo quede un único grupo.\\\\
Es un tipo de procedimiento determinista y voraz (greedy en inglés), ya que realiza las decisiones tomando en cuenta los óptimos locales en cada etapa, esperando obtener con esto un óptimo global.\\\\
Se dice que una partición es más fina (o un refinamiento) de otra partición, si cada grupo de una partición más fina está contenido dentro de un grupo de la partición más gruesa, es decir, cada grupo de la partición más fina es un sub-grupo de un grupo de la partición más gruesa. El agrupamiento jerárquico es un método cuyo resultado es un conjunto de particiones anidadas $P_n, P_{n-1}, ..., P_1$ cada vez más gruesas, donde cada nivel más alto une dos grupos de una partición de un nivel más bajo.\\\\
Para poder realizar este procedimiento, es necesario definir cuan cercanos son dos grupos. Existen diferentes criterios para ello, entre los que cabe mencionar el método de Ward, el de enlace único y el de enlace promedio. En este trabajo, utilizaremos el método de enlace completo (o complete-linkage en inglés), que consiste en tomar la distancia entre dos grupos como el máximo de la distancia entre sus puntos:
\begin{equation}
	D_{cl}(C_i, C_j) = \max\limits_{\vec{x} \in C_i, \vec{y} \in C_j} d(\vec{x}, \vec{y})
\end{equation}
con $d(\vec{x}, \vec{y})$ la función de distancia utilizada para calcular la matriz de disimilaridad entre los elementos.\\\\
Este método permite el manejo de grupos con formas complejas y es invariante ante transformaciones monótonas (como una transformación logarítmica) \cite{Johnson1967} y considera solamente la separación entre elementos, dejando de lado la compacidad o el balance en los grupos.\cite{Shalizi2009, Gan2007}
\subsection{Representación de un agrupamiento jerárquico - dendrogramas}
Un agrupamiento jerárquico puede representarse como un árbol, llamado dendrograma, que permite una rápida interpretación. En un dendrograma, cada nodo está asociado con una altura $h$, tal que si $A$ y $B$ son dos nodos del dendrograma, $h$ cumple:
\begin{equation}
	h(A) \leq h(B) \Leftrightarrow A \subseteq B
\end{equation}
A modo ilustrativo, la figura \ref{fig:agrupamiento_jerarquico} muestra el agrupamiento jerárquico realizado sobre 10 puntos colocados de forma aleatoria en el plano, agrupados utilizando la distancia euclidiana y mediante el método de  enlace completo.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{agrupamiento_jerarquico}
    \caption{Ejemplo de agrupamiento jerárquico realizado sobre 10 puntos colocados de forma aleatoria en el plano, agrupados utilizando la distancia euclidiana y mediante el método de enlace completo. El panel de la izquierda muestra las observaciones, representadas cada una por un número, y el de la derecha el dendrograma obtenido.}
    \label{fig:agrupamiento_jerarquico}
\end{figure}
\section{Detectando grupos en el agrupamiento jerárquico}
\label{sec:grupos_en_agrupamiento_jerarquico}
El agrupamiento jerárquico organiza los objetos en árboles (dendrogramas) cuyas ramas son los grupos deseados. El proceso de detección de grupos a partir de esta estructura se conoce como corte de árbol, corte de ramas o podado de ramas. 
\subsection{Corte de árbol estático}
El método más sencillo de podado es conocido como corte de árbol estático, y funciona definiendo cada rama contigua debajo de una altura fija de corte, como un grupo separado. La cantidad de grupos obtenidos por éste método depende fuertemente de la altura de  corte elegida. La figura \ref{fig:corte_arbol_estatico} muestra dos alturas de corte posibles y los grupos que se obtienen a partir de cada una de ellas.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{corte_arbol_estatico}
    \caption{Ejemplo de corte de árbol a dos alturas diferentes. Al cortar el árbol en $h=3$, se obtienen dos grupos, el grupo $g_1$, que contiene a las observaciones $\{6, 3, 1, 10\}$ y el grupo $g_2$ que contiene a las observaciones $\{2, 4, 8, 5, 7, 9\}$, mientras que al cortarlo en $h=2$, se obtienen cuatro grupos, $g_1'$ con la observación $\{6\}$, $g_2'$ con las observaciones $\{3, 1, 10\}$, $g_3'$ con las observaciones $\{2, 4\}$ y $g_4'$ con las observaciones $\{5, 8, 7, 9\}$.}
    \label{fig:corte_arbol_estatico}
\end{figure}
Al cortar el árbol en $h=3$, se obtienen dos grupos, el grupo $g_1$, que contiene a las observaciones $\{6, 3, 1, 10\}$ y el grupo $g_2$ que contiene a las observaciones $\{2, 4, 8, 5, 7, 9\}$, mientras que al cortarlo en $h=2$, se obtienen cuatro grupos, $g_1'$ con la observación $\{6\}$, $g_2'$ con las observaciones $\{3, 1, 10\}$, $g_3'$ con las observaciones $\{2, 4\}$ y $g_4'$ con las observaciones $\{5, 8, 7, 9\}$.\\\\
Este ejemplo tan sencillo permite poner de manifiesto que el problema del agrupamiento es un problema ``mal planteado'', es decir, cualquier conjunto de puntos puede ser agrupado de maneras drásticamente distintas, sin que exista a priori un único criterio para preferir uno u otro agrupamiento. La fuente de ambigüedades a este respecto más importante, es que la forma en que los datos deberían ser agrupados, depende fuertemente de la \textit{resolución} deseada. Lo que parece una única nube de puntos puede resultar ser, al analizar los datos con mayor resolución, una partición compuesta de muchos grupos. Cada tarea deberá encontrar el nivel adecuado de resolución para obtener la cantidad ``correcta'' de grupos.\cite{Domany1999}\cite{Langfelder2008}
\subsection{Corte de árbol dinámico híbrido}
Si bien es posible detectar grupos distintos en el dendrograma a partir de una inspección visual, utilizar una técnica de corte de árbol estático de forma programática no siempre logra identificar adecuadamente los grupos, ya que al poseer grupos anidados, un solo corte a una altura prefijada no será capaz de detectarlos todos. El método de corte de árbol dinámico híbrido ataca este problema analizando la forma de las ramas del dendrograma en lugar de una altura absoluta \cite{Langfelder2008}. El mismo construye los grupos de abajo hacia arriba en dos pasos. En el primer paso, se detectan las ramas que satisfacen un criterio específico para ser grupos. Este paso de poda está basado en la información de unión del dendrograma. En el segundo paso, se miden cuán cerca de los grupos detectados en el primer paso están todos los objetos no asignados previamente. Si un objeto está suficientemente cerca de un grupo, es asignado a ese grupo. En este paso, se ignora el dendrograma y se utiliza únicamente la información de disimilaridad. Este paso puede considerarse un método modificado de particionado alrededor de medoides (modified Partitioning Around Medoids o mPAM, en inglés). Por eso el nombre de \textit{híbrido}, al tratarse de una mezcla entre agrupamiento jerárquico y no jerárquico.
Los criterios específicos para la detección de grupos se basan en los siguientes cuatro criterios de la forma de las ramas:
\begin{enumerate}
\item Un grupo debe tener una cantidad mínima de objetos.
\item Los objetos que están muy lejos del grupo son excluidos del grupo aunque pertenezcan a la misma rama del dendrograma.
\item Cada grupo debe estar separado de su entorno por una brecha o espacio vacío.
\item El núcleo de cada grupo (el conjunto de objetos con menor altura de unión en el grupo) debe estar fuertemente conectado.
\end{enumerate}
O más formalmente, dado un núcleo de un grupo, llamamos $d$ al promedio de las disimilaridades de a pares entre objetos del núcleo, es decir, a su dispersión y definimos la brecha $g$ de un grupo como la diferencia entre $d$ y la altura donde el grupo se une al resto del dendrograma y entonces, una rama se considera un grupo si:
\begin{enumerate}
\item Tiene al menos $N_0$ objetos.
\item Todas las alturas de unión son a lo sumo de $h_{max}$.
\item La brecha $g$ del grupo es mayor que un $g_{min}$.
\item La dispersión $d$ del núcleo es a lo sumo $d_{max}$.
\end{enumerate}
Los parámetros $N_0$, $h_{max}$, $g_{min}$ y $d_{max}$ son parámetros ajustables del método.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{cut_tree_dynamic_ejemplo}
    \caption{Dendrograma simulado con tres ramas con alturas de unión diferentes. Se observan la altura de corte (cut height), la dispersión del núcleo (core scatter), la brecha (gap) y el núcleo (core). (Fuente: Langfelder y Horvath \cite{Langfelder2008}).}
    \label{fig:cut_tree_dynamic_ejemplo}
\end{figure}
La figura \ref{fig:cut_tree_dynamic_ejemplo} muestra un ejemplo de los parámetros utilizados para definir los grupos en el paso 1.\\\\
Para el paso 2, de tipo PAM, los objetos no asignados (o aquellos grupos que no cumplan tener al menos $N_0$ objetos) son asignados al grupo más cercano si la disimilaridad correspondiente es más pequeña que una disimilaridad máxima definida previamente, o si es más pequeña que el ``radio'' del grupo. El ``radio'' se define como la máxima de las disimilaridades del medoide del grupo al resto de los objetos del mismo.\\\\
Es posible controlar la sensibilidad de las divisiones de los grupos mediante el parámetro \textit{deepSplit}, que puede tomar los valores de 1 a 4. Para un $deepSplit=1$, el método producirá relativamente pocos grupos, de muchos elementos y bien definidos, mientras que para $deepSplit=4$, el método producirá más grupos pero con una dispersión mayor en el núcleo y separado por brechas más pequeñas.\\\\
Para una descripción más detallada del algoritmo, el lector interesado puede referirse a \cite{Langfelder2008}, \cite{Langfelder2007}.

%Sin embargo, por motivos de visualización, solo se pueden representar pequeños sistemas. Las redes reales son usualmente tan grandes que es necesario representarlas mediante algún mecanismo de granularidad más gruesa, es decir, descomponer a la red en módulos que representen varios nodos y arcos. Los módulos son conjuntos de módulos que tienen un alto solapamiento topológico. Este es el objetivo básico de lo que se conoce como \textit{detección de comunidades}.\\\\

\section{Similaridad y agrupamiento en redes}
Como se desarrolló en la sección \ref{sec:redes}, las redes son construcciones útiles para esquematizar la organización de las interacciones en distintos tipos de sistemas. En particular, a partir de estas construcciones es posible estimar nociones de distancia y similaridad entre nodos. 
\subsection{Solapamiento topológico}
Una red puede representarse con una matriz de adyacencia $A=[a_{ij}]$ que codifica que pares de nodos están conectados. Para el caso de enlaces no dirigidos, $A$ es una matriz simétrica donde cada $a_{ij}$ puede tomar un valor entre $[0, 1]$. Para una red no pesada, $0$ indica que dos nodos no están conectados, mientras que $1$ indica que si lo están. Para una red pesada, el elemento de matriz es un número real que indica la fuerza de la conexión.\\\\
A partir de la matriz de adyacencia, es posible construir una matriz de solapamiento topológico $T = [t_{ij}]$ (TOM por sus siglas en inglés) que puede utilizarse como una medida de similaridad para redes biológicas y está definida como:
\begin{equation}
t_{ij} = \begin{cases} 
      \frac{l_{ij}+a_{ij}}{min\{k_i,k_j\}+1-a_{ij}} & i\neq j \\\\
      1 & i=j 
\end{cases}
\end{equation}
donde $l_{ij} = \sum\limits_u a_{iu}a_{uj}$, $k_i = \sum\limits_u a_{iu}$ y $u$ es un índice que recorre todos los nodos de la red.\\\\
El solapamiento topológico de dos nodos refleja su similaridad en términos de los nodos en común que conectan. Básicamente, $t_{ij}$ es un indicador del acuerdo entre el conjunto de nodos vecinos a $i$ con el conjunto de nodos vecinos a $j$. Utilizando esta similaridad, se obtiene una matriz de disimilaridad $D = 1-TOM$ y con esto es posible realizar agrupamientos utilizando, entre otras, alguna de las técnicas antes mencionadas. Además de TOM, es posible definir una matriz de solapamiento topológico generalizada de orden $m$, $T[m] = [t[m]_{ij}]$ (GTOMm), tal que mida el acuerdo entre los nodos que son accesibles por $i$ y por $j$ en $m$ pasos.\cite{Horvath2007}\\\\
\subsection{Detección de comunidades en redes}
\label{sec:comunidades}
En muchos casos, el análisis de patrones de interacción entre partes de un sistema, relevadas en redes complejas permite reconocer la existencia de una organización modular entre sus componentes. Esto también ocurre en muchas redes de origen biológico \cite{Barabasi2004}. De hecho, en los últimos años, muchos trabajos han explorado el vinculo existente entre comunidades de nodos en una red que representa entidades biológicas y la hipotesis de Hartwell acerca de la existencia de ``módulos de funcionalidad biológica'' \cite{Hartwell1999} que postula la existencia de agrupamientos de componentes moleculares, y sus interacciones, capaces de llevar adelante una función biológica especifica. En este trabajo utilizaremos dos métodos de reconocimiento de comunas en redes, Infomap \cite{Rosvall2008} y CNM \cite{Clauset2004}.\\\\
El método o algoritmo Infomap hace uso de criterios de optimización basados en teorías de información, donde los módulos se definen de tal forma que la longitud media de la descripción de un proceso de paseo al azar en el grafo sea mínima, mientras que el desarrollado por Clauset, Newman y Moore que denominaremos CNM, a partir de ciertas heurísticas, busca particiones de la red optimizando directamente una función de calidad $Q$.\\\\
Ambos métodos serán utilizados en este trabajo con el fin de comparar los resultados obtenidos para las comunidades Infomap y CNM con los obtenidos para los métodos de agrupamiento usados.\cite{Berenstein2014, Rosvall2008}\\\\
